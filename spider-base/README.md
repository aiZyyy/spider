
# 网络爬虫的基本概念

* 网络爬虫是什么？
  网络爬虫是一种运行在互联网上用来数据的自动化程序和脚本
* 分解出三个点：
	* 互联网上都有哪些数据？
		* 形形色色的网站组成（新闻|贴吧|知道|音乐|图片|视频|地图|文库）
		* 电商网站（用户|商品|订单|支付|物流|评论|分享）
		* 微博（发送信息|转发|点赞|关注|被关注）
		* ……
	* 怎么去获取？通过什么样的技术手段去获取
		* 网站的本质就是服务端程序，通过客户端（浏览器）和服务端尽心进行交互。
		* HTTP协议（HTTP GET 或者Post ）
		* 开发网络爬虫，就是使用HTTP协议模拟浏览器进行网络请求
		* request 就是提交请求信息
		* response 获得数据
	* 自动化程序或脚本 使用什么语言进行开发
		* 语言是没有任何限制（Python、Java、.net、任何一种）

# 演示一个爬虫程序
	要获取的数据是起点中文网的小说数据、自动化。


# 爬虫究竟有什么用？

* 1）爬虫爬取的数据可以用来做数据分析，扩展：互联网的数据分析，大多数是针对用户的分析
	* 内部数据
	* 外部数据
* 2）爬虫爬去的数据可以给搜索系统使用
	* 内部数据：站内搜索，不用爬虫
	* 外部数据：百度搜索
* 3）竞品分析
* 4）基于数据的商业模式
* 5）……

# 简单分类
* 通用爬虫，爬取的数据量非常大、范围特别广
* 垂直爬虫，只爬取特定领域的数据，小的。


# 网络爬虫运行原理
网络爬虫的本质，其实就是模拟浏览器发送HTTP请求。

http协议的状态码
* 1xx:信息响应类，表示接收到请求并且继续处理
* 2xx:处理成功响应类，表示动作被成功接收、理解和接受
* 3xx:重定向响应类，为了完成指定的动作，必须接受进一步处理
* 4xx:客户端错误，客户请求包含语法错误或者是不能正确执行
* 5xx:服务端错误，服务器不能正确执行一个正确的请求

http协议中常见的状态吗
* 200 正常访问，并且成功
* 301,302 重定向
* 404 资源不存在
* 500 服务器端错误


网络爬虫究竟是怎么运行的？

单个页面是如何运行的？
* 1）指定一个url
* 2）使用技术发送get请求
* 3）获得服务端的响应
* 4）将二进制的数据，转化成HTML文档


网络爬虫一般会爬取很多很多很多的页面
for（）{

* 1）指定一个url
* 2）使用技术发送get请求
* 3）获得服务端的响应
* 4）将二进制的数据，转化成HTML文档

 }

爬虫开发的一个技术点：
* 爬虫中需要一个容器来保存等待爬取的url
* 爬虫中需要一个技术来模拟http请求
* 爬虫中需要将二进制数据转化成html文档（document对象）
* 爬虫中需要一个技术从document对象中解析数据
	* getElementById/ByTagName
	* 解析的数据存放到的哪里？
		* 保存到数据库中，其它的任务地方

两个额外的问题：
等待爬取的url队列中有重复的元素该怎么办？ 需要过滤掉
如果一个页面中包含了其它更多的url，是否要爬取？ 根据需求。

1.5.3爬虫运行的原理
① 指定一个种子url放入到队列中
② 从队列中获取某个URL
③ 使用HTTP协议发起网络请求
④ 在发起网络请求的过程中，需要将域名转化成IP地址，也就是域名解析
⑤ 得到服务器的响应，此时是二进制的输入流
⑥ 将二进制的输入流转换成HTML文档，并解析内容（我们要抓取的内容，比如标题）。
⑦ 将解除出来的内容保持到数据库
⑧ 记录当前URL，并标记为已爬取，避免下次重复爬取。
⑨ 从当前的HTML文档中，解析出页面中包含的其它URL，以供下次爬取
⑩ 判断解析出来的URL是否已经爬取过了，如果已经爬取就丢弃掉
⑪ 将还没爬取过的URL，存放到等待爬取的URL队列中。
⑫ 重复以上的步骤，指导等待爬取的URL队列中没有数据


